
#############################################################

Tacotron Train

###########################################################

Checkpoint path: logs-Tacotron-2/taco_pretrained/tacotron_model.ckpt
Loading training data from: training_data/train.txt
Using model: Tacotron-2
Hyperparameters:
  GL_on_GPU: True
  NN_init: True
  NN_scaler: 0.3
  allow_clipping_in_normalization: True
  attention_dim: 128
  attention_filters: 32
  attention_kernel: (31,)
  attention_win_size: 7
  batch_norm_position: after
  cbhg_conv_channels: 128
  cbhg_highway_units: 128
  cbhg_highwaynet_layers: 4
  cbhg_kernels: 8
  cbhg_pool_size: 2
  cbhg_projection: 256
  cbhg_projection_kernel_size: 3
  cbhg_rnn_units: 128
  cdf_loss: False
  cin_channels: 80
  cleaners: english_cleaners
  clip_for_wavenet: True
  clip_mels_length: True
  clip_outputs: True
  cross_entropy_pos_weight: 1
  cumulative_weights: True
  decoder_layers: 2
  decoder_lstm_units: 1024
  embedding_dim: 512
  enc_conv_channels: 512
  enc_conv_kernel_size: (5,)
  enc_conv_num_layers: 3
  encoder_lstm_units: 256
  fmax: 7600
  fmin: 55
  frame_shift_ms: None
  freq_axis_kernel_size: 3
  gate_channels: 256
  gin_channels: -1
  griffin_lim_iters: 60
  hop_size: 200
  input_type: raw
  kernel_size: 3
  layers: 20
  leaky_alpha: 0.4
  legacy: True
  log_scale_min: -32.23619130191664
  log_scale_min_gauss: -16.11809565095832
  lower_bound_decay: 0.1
  magnitude_power: 2.0
  mask_decoder: False
  mask_encoder: True
  max_abs_value: 4.0
  max_iters: 10000
  max_mel_frames: 900
  max_time_sec: None
  max_time_steps: 11000
  min_level_db: -100
  n_fft: 1024
  n_speakers: 5
  normalize_for_wavenet: True
  num_freq: 513
  num_mels: 80
  out_channels: 2
  outputs_per_step: 1
  postnet_channels: 512
  postnet_kernel_size: (5,)
  postnet_num_layers: 5
  power: 1.5
  predict_linear: True
  preemphasis: 0.97
  preemphasize: True
  prenet_layers: [256, 256]
  quantize_channels: 65536
  ref_level_db: 20
  rescale: True
  rescaling_max: 0.999
  residual_channels: 128
  residual_legacy: True
  sample_rate: 16000
  signal_normalization: True
  silence_threshold: 2
  skip_out_channels: 128
  smoothing: False
  speakers: ['speaker0', 'speaker1', 'speaker2', 'speaker3', 'speaker4']
  speakers_path: None
  split_on_cpu: True
  stacks: 2
  stop_at_any: True
  symmetric_mels: True
  synthesis_constraint: False
  synthesis_constraint_type: window
  tacotron_adam_beta1: 0.9
  tacotron_adam_beta2: 0.999
  tacotron_adam_epsilon: 1e-06
  tacotron_batch_size: 32
  tacotron_clip_gradients: True
  tacotron_data_random_state: 1234
  tacotron_decay_learning_rate: True
  tacotron_decay_rate: 0.5
  tacotron_decay_steps: 18000
  tacotron_dropout_rate: 0.5
  tacotron_final_learning_rate: 0.0001
  tacotron_fine_tuning: False
  tacotron_initial_learning_rate: 0.001
  tacotron_natural_eval: False
  tacotron_num_gpus: 1
  tacotron_random_seed: 5339
  tacotron_reg_weight: 1e-06
  tacotron_scale_regularization: False
  tacotron_start_decay: 40000
  tacotron_swap_with_cpu: False
  tacotron_synthesis_batch_size: 1
  tacotron_teacher_forcing_decay_alpha: None
  tacotron_teacher_forcing_decay_steps: 40000
  tacotron_teacher_forcing_final_ratio: 0.0
  tacotron_teacher_forcing_init_ratio: 1.0
  tacotron_teacher_forcing_mode: constant
  tacotron_teacher_forcing_ratio: 1.0
  tacotron_teacher_forcing_start_decay: 10000
  tacotron_test_batches: None
  tacotron_test_size: 0.05
  tacotron_zoneout_rate: 0.1
  train_with_GTA: True
  trim_fft_size: 2048
  trim_hop_size: 512
  trim_silence: True
  trim_top_db: 40
  upsample_activation: Relu
  upsample_scales: [11, 25]
  upsample_type: SubPixel
  use_bias: True
  use_lws: False
  use_speaker_embedding: True
  wavenet_adam_beta1: 0.9
  wavenet_adam_beta2: 0.999
  wavenet_adam_epsilon: 1e-06
  wavenet_batch_size: 8
  wavenet_clip_gradients: True
  wavenet_data_random_state: 1234
  wavenet_debug_mels: ['training_data/mels/mel-LJ001-0008.npy']
  wavenet_debug_wavs: ['training_data/audio/audio-LJ001-0008.npy']
  wavenet_decay_rate: 0.5
  wavenet_decay_steps: 200000
  wavenet_dropout: 0.05
  wavenet_ema_decay: 0.9999
  wavenet_gradient_max_norm: 100.0
  wavenet_gradient_max_value: 5.0
  wavenet_init_scale: 1.0
  wavenet_learning_rate: 0.001
  wavenet_lr_schedule: exponential
  wavenet_natural_eval: False
  wavenet_num_gpus: 1
  wavenet_pad_sides: 1
  wavenet_random_seed: 5339
  wavenet_swap_with_cpu: False
  wavenet_synth_debug: False
  wavenet_synthesis_batch_size: 20
  wavenet_test_batches: 1
  wavenet_test_size: None
  wavenet_warmup: 4000.0
  wavenet_weight_normalization: False
  win_size: 800
Loaded metadata for 15572 examples (28.57 hours)
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               True
  Eval mode:                False
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 513)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       28.884 Million.
initialisation done /gpu:0
Initialized Tacotron model. Dimensions (? = dynamic shape): 
  Train mode:               False
  Eval mode:                True
  GTA mode:                 False
  Synthesis mode:           False
  Input:                    (?, ?)
  device:                   0
  embedding:                (?, ?, 512)
  enc conv out:             (?, ?, 512)
  encoder out:              (?, ?, 512)
  decoder out:              (?, ?, 80)
  residual out:             (?, ?, 512)
  projected residual out:   (?, ?, 80)
  mel out:                  (?, ?, 80)
  linear out:               (?, ?, 513)
  <stop_token> out:         (?, ?)
  Tacotron Parameters       28.884 Million.
Tacotron training set to a maximum of 100000 steps
No model to load at logs-Tacotron-2/taco_pretrained

Generated 24 test batches of size 32 in 1.200 sec

Generated 64 train batches of size 32 in 2.461 sec
Step       1 [11.258 sec/step, loss=22.21778, avg_loss=22.21778]
Saving Model Character Embeddings visualization..
Tacotron Character embeddings have been updated on tensorboard!
Step       2 [6.322 sec/step, loss=15.49390, avg_loss=18.85584]Exiting due to exception: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[830,32,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node Tacotron_model/inference/CBHG_postnet/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3 (defined at /home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/models/modules.py:77) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Tacotron_model/clip_by_global_norm/mul_63/_1039]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[830,32,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[node Tacotron_model/inference/CBHG_postnet/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3 (defined at /home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/models/modules.py:77) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Original stack trace for 'Tacotron_model/inference/CBHG_postnet/bidirectional_rnn/bw/bw/TensorArrayStack/TensorArrayGatherV3':
  File "train.py", line 138, in <module>
    main()
  File "train.py", line 132, in main
    train(args, log_dir, hparams)
  File "train.py", line 52, in train
    checkpoint = tacotron_train(args, log_dir, hparams)
  File "/home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/train.py", line 399, in tacotron_train
    return train(log_dir, args, hparams)
  File "/home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/train.py", line 156, in train
    model, stats = model_train_mode(args, feeder, hparams, global_step)
  File "/home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/train.py", line 87, in model_train_mode
    is_training=True, split_infos=feeder.split_infos)
  File "/home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/models/tacotron.py", line 210, in initialize
    post_outputs = post_cbhg(mel_outputs, None)
  File "/home/undergraduate/hank/ben/Speech-Synthesis/Tacotron/tacotron/models/modules.py", line 77, in __call__
    dtype=tf.float32)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py", line 503, in bidirectional_dynamic_rnn
    scope=bw_scope)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py", line 324, in new_func
    return func(*args, **kwargs)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py", line 707, in dynamic_rnn
    dtype=dtype)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py", line 920, in _dynamic_rnn_loop
    final_outputs = tuple(ta.stack() for ta in output_final_ta)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py", line 920, in <genexpr>
    final_outputs = tuple(ta.stack() for ta in output_final_ta)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 1205, in stack
    return self._implementation.stack(name=name)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 309, in stack
    return self.gather(math_ops.range(0, self.size()), name=name)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 323, in gather
    element_shape=element_shape)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 6705, in tensor_array_gather_v3
    element_shape=element_shape, name=name)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 788, in _apply_op_helper
    op_def=op_def)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py", line 507, in new_func
    return func(*args, **kwargs)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 3609, in create_op
    op_def=op_def)
  File "/home/undergraduate/anaconda3/envs/steven/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()

